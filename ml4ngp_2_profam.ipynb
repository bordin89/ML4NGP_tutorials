{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtC3R7aciTht"
   },
   "source": [
    "## ðŸ§¬ Generating novel sequences and structures with ProFam-1\n",
    "\n",
    "This notebook presents an end-to-end workflow for **protein sequence generation and structural validation** using **ProFam-1**, a family-aware generative model trained on protein domain and family information. The goal of the workshop is to guide you from an input protein sequence, through controlled generative modelling, to quantitative and visual structural comparison against a reference structure.\n",
    "\n",
    "The notebook is designed to be fully reproducible in a Google Colab environment with GPU support and does not require Flash Attention. All steps are executed interactively, allowing participants to explore how generative parameters affect sequence quality and downstream structural fidelity.\n",
    "\n",
    "We begin by setting up the computational environment and installing the required dependencies. The ProFam repository is cloned, and a pretrained ProFam-1 checkpoint is located and prepared for inference. Care is taken to ensure compatibility with standard Colab GPU instances, avoiding optional components that are not universally available.\n",
    "\n",
    "Next, an input protein sequence is provided. By default, the notebook uses a known PETase sequence from UniProt as a single-sequence prompt, but participants may alternatively paste their own sequence directly into the notebook. Any extraneous whitespace or line breaks are removed automatically, and the sequence is written to FASTA format for downstream use.\n",
    "\n",
    "Using ProFam-1, one or more novel protein sequences are then generated, conditioned on the input sequence. The generation step exposes a small number of adjustable parameters, such as the number of sequences to generate and the sampling temperature, while keeping most model settings fixed to sensible defaults. Additional quality filters are applied to enforce reasonable sequence lengths and minimum similarity to the prompt, helping to avoid pathological or truncated outputs.\n",
    "\n",
    "Structure prediction is performed outside the notebook using a tool such as AlphaFold or ESMFold. Once predicted, the structures of both the generated sequence and the original input sequence are uploaded back into the notebook in either PDB or mmCIF format. These structures form the basis for structural validation.\n",
    "\n",
    "Structural comparison is carried out using **TM-align**, which provides quantitative measures of similarity including TM-score, RMSD, and aligned length. The TM-score is particularly informative: values above 0.5 generally indicate a shared fold, while values above 0.8 suggest strong structural agreement. These metrics allow an objective assessment of whether the generated sequence preserves the overall fold of the reference protein.\n",
    "\n",
    "Finally, the two structures are superposed and visualised together using **NGLView** in cartoon (ribbon/strand) representation. The generated structure is rigidly transformed onto the reference structure using the rotation matrix produced by TM-align, allowing direct visual inspection of conserved cores, flexible regions, and any structural deviations. This final step helps connect quantitative scores to intuitive, three-dimensional understanding.\n",
    "\n",
    "By the end of this notebook, you will have walked through a complete generative protein design loop, from sequence generation to structure-based validation, and gained practical insight into how modern protein language models can be evaluated using structural biology tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtenLPbjgB7d",
    "outputId": "14fca507-b1fa-4dd7-f81c-10176a2931fb"
   },
   "outputs": [],
   "source": [
    "#@title 1) GPU sanity check\n",
    "\n",
    "import torch, subprocess\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version (torch):\", torch.version.cuda)\n",
    "\n",
    "# Optional: show nvidia-smi if available\n",
    "try:\n",
    "    out = subprocess.check_output([\"nvidia-smi\"], text=True)\n",
    "    print(\"\\n\" + out[:1500])\n",
    "except Exception:\n",
    "    print(\"nvidia-smi not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddgjQqXPgocQ",
    "outputId": "22a0c892-ab6a-4503-af28-023e197cf836"
   },
   "outputs": [],
   "source": [
    "#@title 2) Clone ProFam\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "BASE_DIR = pathlib.Path(\"/content\").resolve()\n",
    "WORK_DIR = BASE_DIR / \"profam_workshop\"\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "os.chdir(WORK_DIR)\n",
    "print(\"Working directory:\", WORK_DIR)\n",
    "\n",
    "if not (WORK_DIR / \"profam\").exists():\n",
    "    !git clone https://github.com/alex-hh/profam.git\n",
    "else:\n",
    "    print(\"ProFam repository already present\")\n",
    "\n",
    "%cd {WORK_DIR / \"profam\"}\n",
    "!git rev-parse --short HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgVyS5BKgteH",
    "outputId": "51b1d379-c682-493d-f3d2-081aea2242c6"
   },
   "outputs": [],
   "source": [
    "#@title 3) Install ProFam (GPU runtime, no flash-attn, Colab-friendly)\n",
    "\n",
    "import re, pathlib, os\n",
    "\n",
    "repo = pathlib.Path(\"/content/profam_workshop/profam\").resolve()\n",
    "os.chdir(repo)\n",
    "print(\"Repository:\", repo)\n",
    "\n",
    "# Ensure up-to-date build tools\n",
    "!pip -q install -U pip setuptools wheel\n",
    "\n",
    "req_in = repo / \"requirements.txt\"\n",
    "if not req_in.exists():\n",
    "    raise FileNotFoundError(f\"Missing {req_in}\")\n",
    "\n",
    "# Packages we deliberately do not override in Colab\n",
    "SKIP_PREFIXES = (\n",
    "    \"flash-attn\", \"flash_attn\",\n",
    "    \"numpy\", \"pandas\", \"requests\",\n",
    "    \"torch\", \"torchvision\", \"torchaudio\",\n",
    "    \"jax\", \"jaxlib\",\n",
    ")\n",
    "\n",
    "filtered = []\n",
    "removed = []\n",
    "\n",
    "for ln in req_in.read_text().splitlines():\n",
    "    s = ln.strip()\n",
    "    if not s or s.startswith(\"#\"):\n",
    "        filtered.append(ln)\n",
    "        continue\n",
    "\n",
    "    head = re.split(r\"[<>=!\\s\\[]\", s, maxsplit=1)[0].lower()\n",
    "\n",
    "    if any(head.startswith(p) for p in SKIP_PREFIXES):\n",
    "        removed.append(ln)\n",
    "        continue\n",
    "\n",
    "    filtered.append(ln)\n",
    "\n",
    "req_out = repo / \"requirements.colab_noflash.txt\"\n",
    "req_out.write_text(\"\\n\".join(filtered) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {req_out.name}\")\n",
    "\n",
    "if removed:\n",
    "    print(\"Skipped packages:\")\n",
    "    for r in removed:\n",
    "        print(\" \", r)\n",
    "\n",
    "# Install filtered dependencies\n",
    "!pip -q install -r requirements.colab_noflash.txt\n",
    "\n",
    "# Patch common Colab/runtime dependencies\n",
    "!pip -q install \\\n",
    "    \"pandas==2.2.2\" \\\n",
    "    \"requests==2.32.4\" \\\n",
    "    \"packaging>=24.2\" \\\n",
    "    \"typing-extensions>=4.12.0\" \\\n",
    "    \"xxhash>=3.5.0\" \\\n",
    "    \"jedi>=0.16\"\n",
    "\n",
    "# Install ProFam in editable mode\n",
    "!pip -q install -e .\n",
    "\n",
    "print(\"ProFam installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AE6Ezw9rIHX",
    "outputId": "253293d4-0ab8-44fb-9bb3-d073d7071c88"
   },
   "outputs": [],
   "source": [
    "#@title 4) Download ProFam model checkpoint from Hugging Face\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "repo = pathlib.Path(\"/content/profam_workshop/profam\").resolve()\n",
    "os.chdir(repo)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
    "pathlib.Path(os.environ[\"HF_HOME\"]).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "!python scripts/hf_download_checkpoint.py\n",
    "\n",
    "print(\"Checkpoint download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRl_-dT7xpC4",
    "outputId": "613aafae-f3a4-4928-b1a9-cc3103b20dbb"
   },
   "outputs": [],
   "source": [
    "#@title 5) Fix Colab torch/torchvision mismatch (remove torchvision)\n",
    "\n",
    "# Remove torchvision to prevent torchmetrics/lightning import issues\n",
    "!pip -q uninstall -y torchvision || true\n",
    "\n",
    "print(\"torchvision removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkCDYcH-u5Vt",
    "outputId": "155f8356-df54-4923-c6e8-b3376480c2cc"
   },
   "outputs": [],
   "source": [
    "#@title 6) Show ProFam CLI help (score + generate)\n",
    "\n",
    "import pathlib, os\n",
    "\n",
    "repo = pathlib.Path(\"/content/profam_workshop/profam\").resolve()\n",
    "os.chdir(repo)\n",
    "\n",
    "print(\"=== score_sequences.py ===\")\n",
    "!python scripts/score_sequences.py -h | head -n 120\n",
    "\n",
    "print(\"\\n=== generate_sequences.py ===\")\n",
    "!python scripts/generate_sequences.py -h | head -n 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkZQIvrhzZox",
    "outputId": "b18495be-d009-4e65-8c3c-8b50c83c6057"
   },
   "outputs": [],
   "source": [
    "#@title 7) Create prompt FASTA (default PETase or user-provided sequence)\n",
    "\n",
    "import pathlib, textwrap, re, os\n",
    "\n",
    "repo = pathlib.Path(\"/content/profam_workshop/profam\").resolve()\n",
    "os.chdir(repo)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DISCLAIMER\n",
    "# -----------------------------------------------------------------------------\n",
    "# ProFam can also take a multi-sequence FASTA or an aligned MSA (e.g. A3M).\n",
    "#\n",
    "# - A single sequence is sufficient and recommended for sequence generation.\n",
    "# - Multiple FASTA entries are treated as independent prompts.\n",
    "# - Providing an MSA can be used to condition generation on a protein family,\n",
    "#   but alignment is NOT required for generation.\n",
    "# - MSAs are primarily important for variant scoring, not for generation.\n",
    "#\n",
    "# For this workshop, we use a single-sequence prompt by default.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =========================\n",
    "# USER CHOICES\n",
    "# =========================\n",
    "USE_DEFAULT = True  #@param {type:\"boolean\"}\n",
    "\n",
    "# If USE_DEFAULT = False, paste either:\n",
    "#  - FASTA (one or more records), OR\n",
    "#  - a raw amino-acid sequence\n",
    "USER_SEQUENCE_TEXT = \"MNKFLALALAVSLSASAAPVPSQAFGDLGKDTVAV GDSGVPVSPQTDPSATVGRRLTAAALDALDAGADVV VPGSAGTFSVTLGATNATVVGVDLQLAGADATVTLA AGATGNSGGYVVWGGHGTQATQVVAGLPQLAVAGAD VVIVDNNRAGADVVAVSGGTTSTTTW\"  #@param {type:\"string\"}\n",
    "\n",
    "USER_SEQ_NAME = \"TEST\"  #@param {type:\"string\"}\n",
    "\n",
    "out_fa = repo / \"prompts.fasta\"\n",
    "\n",
    "# =========================\n",
    "# DEFAULT: PETase (Ideonella sakaiensis)\n",
    "# UniProt: QRG82925\n",
    "# =========================\n",
    "DEFAULT_FASTA = textwrap.dedent(\"\"\"\\\n",
    ">PETase_QRG82925\n",
    "MNFPRASRLMQAAVLGGLMAVSAAATAQTNPYARGPNPTAASLEASAGPFTVRSFTVSRP\n",
    "SGYGAGTVYYPTNAGGTVGAIAIVPGYTARQSSIKWWGPRLASHGFVVITIDTNSTLDQP\n",
    "SSRSSQQMAALRQVASLNGTSSSPIYGKVDTARMGVMGWSMGGGGSLISAANNPSLKAAA\n",
    "PQAPWDSSTNFSSVTVPTLIFACENDSIAPVNSSALPIYDSMSRNAKQFLEINGGSHSCA\n",
    "NSGNSNQALIGKKGVAWMKRFMDNDTRYSTFACENPNSTRVSDFRTANCS\n",
    "\"\"\")\n",
    "\n",
    "AA_RE = re.compile(r\"^[ACDEFGHIKLMNPQRSTVWYBXZJUO\\-\\.\\s]+$\", re.IGNORECASE)\n",
    "\n",
    "def normalize_raw_sequence(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Accept raw amino-acid sequences that may contain spaces or newlines.\n",
    "    Strip everything except letters Aâ€“Z and uppercase.\n",
    "    \"\"\"\n",
    "    s = s.strip().upper()\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "    s = re.sub(r\"[^A-Z]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def to_fasta_from_raw(seq: str, name: str) -> str:\n",
    "    seq = normalize_raw_sequence(seq)\n",
    "    if not seq:\n",
    "        raise ValueError(\"Sequence is empty after cleaning.\")\n",
    "    wrapped = \"\\n\".join(seq[i:i+60] for i in range(0, len(seq), 60))\n",
    "    return f\">{name}\\n{wrapped}\\n\"\n",
    "\n",
    "def is_fasta(txt: str) -> bool:\n",
    "    return txt.lstrip().startswith(\">\")\n",
    "\n",
    "# =========================\n",
    "# WRITE FASTA\n",
    "# =========================\n",
    "if USE_DEFAULT:\n",
    "    out_fa.write_text(DEFAULT_FASTA)\n",
    "    print(\"Using default PETase sequence (UniProt QRG82925)\")\n",
    "else:\n",
    "    txt = (USER_SEQUENCE_TEXT or \"\").strip()\n",
    "    if not txt:\n",
    "        raise ValueError(\"USE_DEFAULT is False but USER_SEQUENCE_TEXT is empty.\")\n",
    "\n",
    "    if is_fasta(txt):\n",
    "        out_fa.write_text(txt.strip() + \"\\n\")\n",
    "        print(\"Using user-provided FASTA (single or multiple sequences)\")\n",
    "    else:\n",
    "        if not AA_RE.match(txt.replace(\"\\n\", \"\")):\n",
    "            raise ValueError(\"Input is neither FASTA nor a valid amino-acid sequence.\")\n",
    "        out_fa.write_text(to_fasta_from_raw(txt, USER_SEQ_NAME))\n",
    "        print(\"Using user-provided raw sequence\")\n",
    "\n",
    "print(\"\\nNote:\")\n",
    "print(\"This FASTA can contain a single sequence, multiple sequences, or an MSA.\")\n",
    "print(\"For generation, alignment is optional; for scoring, MSAs are recommended.\")\n",
    "\n",
    "print(\"\\n--- prompts.fasta ---\")\n",
    "print(out_fa.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K01hNFNM0Ejb",
    "outputId": "ffe181d3-dd4b-456c-fa2e-8ccb1b24eee5"
   },
   "outputs": [],
   "source": [
    "#@title 8) Locate ProFam checkpoint_dir\n",
    "\n",
    "import pathlib, os\n",
    "\n",
    "repo = pathlib.Path(\"/content/profam_workshop/profam\").resolve()\n",
    "os.chdir(repo)\n",
    "\n",
    "candidates = []\n",
    "\n",
    "# Look inside the repository\n",
    "for p in pathlib.Path(\".\").rglob(\".hydra\"):\n",
    "    candidates.append(p.parent)\n",
    "\n",
    "# Look in Hugging Face cache\n",
    "hf_home = pathlib.Path(os.environ.get(\"HF_HOME\", \"/content/hf_cache\"))\n",
    "if hf_home.exists():\n",
    "    for p in hf_home.rglob(\".hydra\"):\n",
    "        candidates.append(p.parent)\n",
    "\n",
    "# De-duplicate\n",
    "uniq = []\n",
    "seen = set()\n",
    "for c in candidates:\n",
    "    c = c.resolve()\n",
    "    if c not in seen:\n",
    "        seen.add(c)\n",
    "        uniq.append(c)\n",
    "\n",
    "if not uniq:\n",
    "    raise RuntimeError(\n",
    "        \"Could not find a checkpoint directory containing '.hydra'. \"\n",
    "        \"Run the checkpoint download cell first.\"\n",
    "    )\n",
    "\n",
    "# Select the most recent checkpoint\n",
    "checkpoint_dir = sorted(uniq, key=lambda x: x.stat().st_mtime, reverse=True)[0]\n",
    "print(\"Using checkpoint_dir:\", checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nTvcavD0P9k"
   },
   "source": [
    "# Generate Sequences using ProFam-1\n",
    "\n",
    "In this section, we use **ProFam-1**, a protein familyâ€“aware generative model, to generate new protein sequences from a single input sequence.\n",
    "\n",
    "The model conditions on the input sequence and samples novel sequences that are consistent with the learned protein family constraints.\n",
    "\n",
    "**Notes for the workshop:**\n",
    "- The generation runs on **GPU** if available.\n",
    "- **Flash Attention is disabled** for compatibility with Google Colab.\n",
    "- Output sequences will be written as FASTA files in the `generated/` directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0-MjOQO01-o"
   },
   "source": [
    "> **Runtime & Resource Notice**\n",
    ">\n",
    "> - Sequence generation may take **1â€“3 minutes** on a Google Colab GPU.\n",
    "> - The first run may be slightly slower due to **model weight loading**.\n",
    "> - If you see CUDA or cuDNN warnings, these are expected and can be ignored.\n",
    ">\n",
    "> If the notebook appears idle, please wait â€” the model is still running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59V1M2PH04E2"
   },
   "source": [
    "## Generation Parameters\n",
    "\n",
    "The sequence generation step uses probabilistic sampling. The key parameters are:\n",
    "\n",
    "| Parameter | Meaning |\n",
    "|---------|--------|\n",
    "| `temperature` | Controls randomness. Lower = more conservative, higher = more diverse |\n",
    "| `top_p` | Nucleus sampling threshold (keeps top probability mass) |\n",
    "| `num_samples` | Number of sequences generated per prompt |\n",
    "| `max_tokens` | Maximum total tokens generated |\n",
    "| `sampler` | `ensemble` uses multiple internal prompts for robustness |\n",
    "| `device` | `cuda` uses GPU if available |\n",
    "| `dtype` | `float16` reduces memory usage on GPU |\n",
    "| `attn_implementation` | `sdpa` is used instead of Flash Attention for Colab compatibility |\n",
    "\n",
    "**Tip:**  \n",
    "For more diversity, increase `temperature` (e.g. `1.2`).  \n",
    "For safer, more conservative sequences, decrease it (e.g. `0.8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgF_3pAUCdUQ",
    "outputId": "5b10514f-3468-466c-9130-ca12fe309386"
   },
   "outputs": [],
   "source": [
    "#@title 9) Install MMseqs2 (required for --minimum_sequence_identity filtering)\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "WORK_DIR = pathlib.Path(\"/content/profam_workshop\").resolve()\n",
    "mmseqs_dir = WORK_DIR / \"mmseqs\"\n",
    "mmseqs_bin = mmseqs_dir / \"bin\" / \"mmseqs\"\n",
    "\n",
    "# Clean and install\n",
    "!rm -rf \"{mmseqs_dir}\"\n",
    "!mkdir -p \"{mmseqs_dir}\"\n",
    "!wget -q https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz -O /tmp/mmseqs.tar.gz\n",
    "!tar -xzf /tmp/mmseqs.tar.gz -C \"{mmseqs_dir}\" --strip-components=1\n",
    "!rm /tmp/mmseqs.tar.gz\n",
    "\n",
    "# Add to PATH\n",
    "os.environ[\"PATH\"] = f\"{mmseqs_dir}/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "print(\"mmseqs binary:\", mmseqs_bin)\n",
    "!{mmseqs_bin} version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPMylnhyqXsA",
    "outputId": "0d3f5daf-8f24-4067-95dc-d7cde36cc54c"
   },
   "outputs": [],
   "source": [
    "#@title 10) Device & dtype selection (GPU with safe fallback)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Preferred defaults (workshop intent)\n",
    "PREFERRED_DEVICE = \"cuda\"\n",
    "PREFERRED_DTYPE  = \"float16\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = PREFERRED_DEVICE\n",
    "    dtype  = PREFERRED_DTYPE\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype  = \"float32\"\n",
    "    print(\n",
    "        \"No NVIDIA GPU detected.\\n\"\n",
    "        \"Falling back to CPU (this will be slower, but functional).\"\n",
    "    )\n",
    "\n",
    "print(f\"device = {device}\")\n",
    "print(f\"dtype  = {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOI6swA61QnR",
    "outputId": "c538e93d-ecac-4917-eccb-55fb03bbb048"
   },
   "outputs": [],
   "source": [
    "#@title 11) Generate sequences with ProFam-1\n",
    "\n",
    "import pathlib, os, shutil\n",
    "import torch\n",
    "\n",
    "# ---------------- User parameters ----------------\n",
    "num_sequences = 10  #@param {type:\"integer\", min:1, max:50, step:1}\n",
    "temperature   = 0.8  #@param {type:\"number\", min:0.5, max:1.5, step:0.1}\n",
    "\n",
    "max_tokens = 2048  #@param {type:\"integer\", min:256, max:8192, step:256}\n",
    "max_generated_length = 512  #@param {type:\"integer\", min:64, max:4096, step:64}\n",
    "\n",
    "min_seq_len_prop = 0.8   #@param {type:\"number\", min:0.5, max:1.0, step:0.05}\n",
    "max_len_mult     = 1.2   #@param {type:\"number\", min:1.0, max:2.0, step:0.1}\n",
    "min_seq_identity = 0.8   #@param {type:\"number\", min:0.0, max:0.9, step:0.05}\n",
    "\n",
    "# ---------------- Fixed workshop defaults ----------------\n",
    "top_p      = 0.95\n",
    "sampler    = \"ensemble\"\n",
    "attn_impl  = \"sdpa\"   # no flash-attn\n",
    "seed       = 1\n",
    "\n",
    "# ---------------- Auto device / dtype ----------------\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    dtype  = \"float16\"\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype  = \"float32\"\n",
    "    print(\"No GPU detected; using CPU (slower).\")\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "repo = pathlib.Path(\"/content/profam_workshop/profam\").resolve()\n",
    "os.chdir(repo)\n",
    "\n",
    "prompt_fa = repo / \"prompts.fasta\"\n",
    "save_dir  = repo / \"generated\"\n",
    "\n",
    "if not prompt_fa.exists():\n",
    "    raise FileNotFoundError(\"prompts.fasta not found. Run the prompt FASTA cell first.\")\n",
    "\n",
    "if \"checkpoint_dir\" not in globals():\n",
    "    raise RuntimeError(\"checkpoint_dir not set. Run the checkpoint discovery cell first.\")\n",
    "\n",
    "# Overwrite previous results\n",
    "if save_dir.exists():\n",
    "    shutil.rmtree(save_dir)\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------------- Run generation ----------------\n",
    "cmd = (\n",
    "    f\"python scripts/generate_sequences.py \"\n",
    "    f\"--checkpoint_dir '{checkpoint_dir}' \"\n",
    "    f\"--file_path '{prompt_fa}' \"\n",
    "    f\"--save_dir '{save_dir}' \"\n",
    "    f\"--sampler {sampler} \"\n",
    "    f\"--num_prompts_in_ensemble 1 \"\n",
    "    f\"--num_samples {num_sequences} \"\n",
    "    f\"--max_tokens {max_tokens} \"\n",
    "    f\"--max_generated_length {max_generated_length} \"\n",
    "    f\"--temperature {temperature} \"\n",
    "    f\"--top_p {top_p} \"\n",
    "    f\"--device {device} \"\n",
    "    f\"--dtype {dtype} \"\n",
    "    f\"--attn_implementation {attn_impl} \"\n",
    "    f\"--minimum_sequence_length_proportion {min_seq_len_prop} \"\n",
    "    f\"--max_sequence_length_multiplier {max_len_mult} \"\n",
    "    f\"--seed {seed}\"\n",
    ")\n",
    "\n",
    "# Only enable identity filter when mmseqs exists\n",
    "mmseqs_ok = shutil.which(\"mmseqs\") is not None\n",
    "if mmseqs_ok:\n",
    "    cmd += f\" --minimum_sequence_identity {min_seq_identity}\"\n",
    "else:\n",
    "    print(\"mmseqs not found in PATH; skipping --minimum_sequence_identity filtering.\")\n",
    "\n",
    "print(\"\\nCommand:\\n\", cmd, \"\\n\")\n",
    "!bash -lc \"{cmd}\"\n",
    "\n",
    "print(\"\\nGenerated files:\")\n",
    "!ls -lh \"{save_dir}\" | head -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gEH2TpMt1sVx",
    "outputId": "952a791b-581d-4812-9c50-b219bcf077d4"
   },
   "outputs": [],
   "source": [
    "#@title 12) Preview generated FASTA (first file)\n",
    "\n",
    "import pathlib\n",
    "\n",
    "repo = pathlib.Path(\"/content/profam_workshop/profam\").resolve()\n",
    "save_dir = repo / \"generated\"\n",
    "\n",
    "fasta_files = sorted(save_dir.glob(\"*.fa*\"))\n",
    "if not fasta_files:\n",
    "    raise RuntimeError(\"No FASTA files found in generated/. Run the generation cell first.\")\n",
    "\n",
    "fp = fasta_files[0]\n",
    "print(\"Showing:\", fp, \"\\n\")\n",
    "print(\"\\n\".join(fp.read_text().splitlines()[:120]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "m-pK0YQKBFk0",
    "outputId": "7ae076fd-9300-47b0-d519-8f35b33bb488"
   },
   "outputs": [],
   "source": [
    "#@title 13) Rank generated sequences and pick the best candidate\n",
    "\n",
    "import pathlib, re\n",
    "import pandas as pd\n",
    "\n",
    "repo = pathlib.Path(\"/content/profam_workshop/profam\").resolve()\n",
    "prompt_fa = repo / \"prompts.fasta\"\n",
    "gen_dir = repo / \"generated\"\n",
    "out_best = repo / \"best_generated.fasta\"\n",
    "\n",
    "if not prompt_fa.exists():\n",
    "    raise FileNotFoundError(\"prompts.fasta not found.\")\n",
    "if not gen_dir.exists():\n",
    "    raise FileNotFoundError(\"generated/ directory not found. Run generation first.\")\n",
    "\n",
    "AA = set(\"ACDEFGHIKLMNPQRSTVWYBXZJUO\")\n",
    "\n",
    "def read_fasta(path: pathlib.Path):\n",
    "    records = []\n",
    "    name, seq = None, []\n",
    "    for line in path.read_text().splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(\">\"):\n",
    "            if name is not None:\n",
    "                records.append((name, \"\".join(seq)))\n",
    "            name = line[1:].strip()\n",
    "            seq = []\n",
    "        else:\n",
    "            seq.append(line)\n",
    "    if name is not None:\n",
    "        records.append((name, \"\".join(seq)))\n",
    "    return records\n",
    "\n",
    "def clean_seq(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"\", s).upper()\n",
    "    return \"\".join(c for c in s if c.isalpha())\n",
    "\n",
    "def longest_run_fraction(s: str) -> float:\n",
    "    if not s:\n",
    "        return 0.0\n",
    "    best = 1\n",
    "    cur = 1\n",
    "    for i in range(1, len(s)):\n",
    "        if s[i] == s[i - 1]:\n",
    "            cur += 1\n",
    "            if cur > best:\n",
    "                best = cur\n",
    "        else:\n",
    "            cur = 1\n",
    "    return best / len(s)\n",
    "\n",
    "def kmer_low_complexity(s: str, k: int = 3) -> float:\n",
    "    if len(s) < k + 1:\n",
    "        return 0.0\n",
    "    kmers = [s[i:i + k] for i in range(len(s) - k + 1)]\n",
    "    return 1.0 - (len(set(kmers)) / len(kmers))\n",
    "\n",
    "def identity_to_prompt(g: str, p: str) -> float:\n",
    "    n = min(len(g), len(p))\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    matches = sum(1 for i in range(n) if g[i] == p[i])\n",
    "    return matches / n\n",
    "\n",
    "prompt_records = read_fasta(prompt_fa)\n",
    "if not prompt_records:\n",
    "    raise RuntimeError(\"No FASTA records found in prompts.fasta.\")\n",
    "prompt_name, prompt_seq = prompt_records[0][0], clean_seq(prompt_records[0][1])\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(gen_dir.glob(\"*.fa*\")):\n",
    "    for name, seq in read_fasta(fp):\n",
    "        seq = clean_seq(seq)\n",
    "        if not seq:\n",
    "            continue\n",
    "        if any(c not in AA for c in seq):\n",
    "            continue\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"file\": fp.name,\n",
    "                \"id\": name,\n",
    "                \"length\": len(seq),\n",
    "                \"len_ratio_to_prompt\": len(seq) / max(1, len(prompt_seq)),\n",
    "                \"prefix_identity_to_prompt\": identity_to_prompt(seq, prompt_seq),\n",
    "                \"longest_run_frac\": longest_run_fraction(seq),\n",
    "                \"kmer_low_complexity\": kmer_low_complexity(seq, k=3),\n",
    "                \"seq\": seq,\n",
    "            }\n",
    "        )\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No generated sequences found/parsed in generated/.\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "target_min_id = 0.20\n",
    "target_max_id = 0.60\n",
    "\n",
    "def rank_score(r):\n",
    "    len_pen = abs(r[\"len_ratio_to_prompt\"] - 1.0)\n",
    "\n",
    "    idv = r[\"prefix_identity_to_prompt\"]\n",
    "    if idv < target_min_id:\n",
    "        id_pen = (target_min_id - idv) * 2.0\n",
    "    elif idv > target_max_id:\n",
    "        id_pen = (idv - target_max_id) * 2.0\n",
    "    else:\n",
    "        id_pen = 0.0\n",
    "\n",
    "    rep_pen = r[\"kmer_low_complexity\"] * 1.5 + r[\"longest_run_frac\"] * 2.0\n",
    "\n",
    "    return len_pen + id_pen + rep_pen\n",
    "\n",
    "df[\"rank_score\"] = df.apply(rank_score, axis=1)\n",
    "df_sorted = df.sort_values(\"rank_score\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "display_cols = [\n",
    "    \"file\",\n",
    "    \"id\",\n",
    "    \"length\",\n",
    "    \"len_ratio_to_prompt\",\n",
    "    \"prefix_identity_to_prompt\",\n",
    "    \"kmer_low_complexity\",\n",
    "    \"longest_run_frac\",\n",
    "    \"rank_score\",\n",
    "]\n",
    "display(df_sorted[display_cols].head(15))\n",
    "\n",
    "best = df_sorted.iloc[0]\n",
    "best_id = best[\"id\"]\n",
    "best_seq = best[\"seq\"]\n",
    "\n",
    "wrapped = \"\\n\".join(best_seq[i:i + 60] for i in range(0, len(best_seq), 60))\n",
    "out_best.write_text(f\">{best_id}\\n{wrapped}\\n\")\n",
    "\n",
    "print(\"Recommended sequence:\")\n",
    "print(\"  ID:\", best_id)\n",
    "print(\"  Length:\", len(best_seq))\n",
    "print(\"  Score:\", float(best[\"rank_score\"]))\n",
    "print(\"  Saved:\", out_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "0tSjFaKYHzzy",
    "outputId": "30de26f6-f58a-406d-8744-80676ad2fc51"
   },
   "outputs": [],
   "source": [
    "#@title 14) Upload two structures for TM-score and visualisation\n",
    "\n",
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "UPLOAD_DIR = Path(\"/content/tmalign_uploads\")\n",
    "UPLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def upload_one(label):\n",
    "    print(f\"\\nUpload {label} (PDB .pdb/.ent or mmCIF .cif/.mmcif)\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    if len(uploaded) != 1:\n",
    "        raise RuntimeError(\n",
    "            f\"{label}: Please upload exactly one file (got {len(uploaded)}). \"\n",
    "            \"Re-run the cell.\"\n",
    "        )\n",
    "\n",
    "    name, data = next(iter(uploaded.items()))\n",
    "    dst = UPLOAD_DIR / name\n",
    "\n",
    "    with open(dst, \"wb\") as f:\n",
    "        f.write(data)\n",
    "\n",
    "    print(f\"{label} saved to: {dst}\")\n",
    "    return str(dst)\n",
    "\n",
    "# Upload structure A\n",
    "struct_a = upload_one(\"Structure A (e.g. ProFam-generated model)\")\n",
    "\n",
    "# Upload structure B\n",
    "struct_b = upload_one(\"Structure B (e.g. AFDB or reference structure)\")\n",
    "\n",
    "print(\"\\nStructures ready for TM-score:\")\n",
    "print(\"  Structure A:\", struct_a)\n",
    "print(\"  Structure B:\", struct_b)\n",
    "print(\"\\nRun the TM-score alignment cell next.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490,
     "referenced_widgets": [
      "2f5c908b47c34249a826f542f5adf054",
      "c1b930a01abf44a086ff2fad756f0b94",
      "4dcfe5e1f00a469d8696518d42d95c0e",
      "b7bef3eeb335450292f0b76a43a54dc0",
      "c6370a6952144c73bf7fdc39cf3d8a3b",
      "f3712a1a444d4cd293b7565cc305c0c1",
      "3233db53b097403a95ade1daf78e1fbb",
      "f8ce6b8f649e4c35bca89fbb573e61da",
      "1516b48783174c179c0072f2e152b2fe",
      "d432e843235d47d0b4e4c7344a29ef37",
      "a317ad2cea59403383d87adbddafb780",
      "c8d25d4fac8541c1a98a4dde6d1b62af",
      "da738cfd54bd4cdaad4fb0c906f29106"
     ]
    },
    "id": "hgBnxxixT-3T",
    "outputId": "b515fa7e-a771-4aac-c84b-3371e6b7ce3d"
   },
   "outputs": [],
   "source": [
    "#@title TM-align: align A vs B, report TM-score, and visualise superposition (single output)\n",
    "\n",
    "import re, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "SHOW_3DMOL = False  #@param {type:\"boolean\"}  # If True, shows 3Dmol output before NGLView\n",
    "\n",
    "# --- Inputs from upload cells ------------------------------------------------\n",
    "if \"struct_a\" not in globals() or \"struct_b\" not in globals():\n",
    "    raise RuntimeError(\"struct_a / struct_b not set. Run the upload cell first.\")\n",
    "\n",
    "struct_a = str(struct_a)\n",
    "struct_b = str(struct_b)\n",
    "\n",
    "chain_a = \"\"  #@param {type:\"string\"}  # leave blank = first chain\n",
    "chain_b = \"\"  #@param {type:\"string\"}  # leave blank = first chain\n",
    "\n",
    "WORK_DIR = Path(\"/content/tmalign_work\").resolve()\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Install deps (quiet)\n",
    "!pip -q install gemmi nglview\n",
    "if SHOW_3DMOL:\n",
    "    !pip -q install py3Dmol\n",
    "\n",
    "# Enable widget manager for NGLView in Colab\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "import gemmi\n",
    "import nglview as nv\n",
    "\n",
    "def ensure_tmalign(bin_path: Path) -> Path:\n",
    "    if bin_path.exists():\n",
    "        return bin_path\n",
    "    bin_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    url = \"https://zhanggroup.org/TM-align/TMalign\"\n",
    "    subprocess.run([\"bash\", \"-lc\", f\"wget -q '{url}' -O '{bin_path}'\"], check=True)\n",
    "    subprocess.run([\"bash\", \"-lc\", f\"chmod +x '{bin_path}'\"], check=True)\n",
    "    return bin_path\n",
    "\n",
    "TMALIGN = ensure_tmalign(WORK_DIR / \"bin\" / \"TMalign\")\n",
    "\n",
    "def first_chain_id(path: str) -> str:\n",
    "    st = gemmi.read_structure(path)\n",
    "    for model in st:\n",
    "        for ch in model:\n",
    "            return ch.name\n",
    "    return \"\"\n",
    "\n",
    "def to_pdb(in_path: str, out_path: Path, chain_id: str = \"\") -> Path:\n",
    "    st = gemmi.read_structure(str(in_path))\n",
    "    if chain_id.strip():\n",
    "        keep = chain_id.strip()\n",
    "        for model in st:\n",
    "            for ch in list(model):\n",
    "                if ch.name != keep:\n",
    "                    model.remove_chain(ch.name)\n",
    "        st.remove_empty_chains()\n",
    "    st.write_pdb(str(out_path))\n",
    "    return out_path\n",
    "\n",
    "# Choose default chains if blank\n",
    "if chain_a.strip() == \"\":\n",
    "    chain_a = first_chain_id(struct_a)\n",
    "if chain_b.strip() == \"\":\n",
    "    chain_b = first_chain_id(struct_b)\n",
    "\n",
    "# Prepare inputs\n",
    "pdb_a = WORK_DIR / \"A.pdb\"\n",
    "pdb_b = WORK_DIR / \"B.pdb\"\n",
    "to_pdb(struct_a, pdb_a, chain_a)\n",
    "to_pdb(struct_b, pdb_b, chain_b)\n",
    "\n",
    "# --- Run TM-align with matrix output -----------------------------------------\n",
    "matrix_file = WORK_DIR / \"tmalign_matrix.txt\"\n",
    "if matrix_file.exists():\n",
    "    matrix_file.unlink()\n",
    "\n",
    "res = subprocess.run(\n",
    "    [str(TMALIGN), str(pdb_a), str(pdb_b), \"-m\", str(matrix_file)],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "out = (res.stdout or \"\") + \"\\n\" + (res.stderr or \"\")\n",
    "if res.returncode != 0:\n",
    "    print(out)\n",
    "    raise RuntimeError(f\"TM-align failed with exit code {res.returncode}\")\n",
    "\n",
    "# --- Parse summary ------------------------------------------------------------\n",
    "tm_scores = [float(x) for x in re.findall(r\"TM-score=\\s*([0-9]*\\.[0-9]+)\", out)]\n",
    "rmsd_m = re.search(r\"RMSD=\\s*([0-9]*\\.[0-9]+)\", out)\n",
    "aln_m  = re.search(r\"Aligned length=\\s*(\\d+)\", out)\n",
    "\n",
    "print(\"TM-align summary\")\n",
    "print(\"  A:\", Path(struct_a).name, f\"(chain {chain_a})\")\n",
    "print(\"  B:\", Path(struct_b).name, f\"(chain {chain_b})\")\n",
    "if aln_m:\n",
    "    print(f\"  Aligned length: {int(aln_m.group(1))}\")\n",
    "if rmsd_m:\n",
    "    print(f\"  RMSD: {float(rmsd_m.group(1)):.3f}\")\n",
    "if tm_scores:\n",
    "    if len(tm_scores) >= 2:\n",
    "        print(f\"  TM-score (norm by A length): {tm_scores[0]:.5f}\")\n",
    "        print(f\"  TM-score (norm by B length): {tm_scores[1]:.5f}\")\n",
    "        print(f\"  TM-score (max): {max(tm_scores[0], tm_scores[1]):.5f}\")\n",
    "    else:\n",
    "        print(f\"  TM-score: {tm_scores[0]:.5f}\")\n",
    "print()\n",
    "\n",
    "# --- Robust parse of 3x4 matrix from -m file ---------------------------------\n",
    "if not matrix_file.exists() or matrix_file.stat().st_size == 0:\n",
    "    raise RuntimeError(\"TM-align did not create a matrix file. Something went wrong with -m.\")\n",
    "\n",
    "lines = [ln.strip() for ln in matrix_file.read_text().splitlines() if ln.strip()]\n",
    "\n",
    "rows = {}\n",
    "for ln in lines:\n",
    "    parts = ln.split()\n",
    "    if len(parts) >= 5 and parts[0] in (\"1\", \"2\", \"3\"):\n",
    "        m = int(parts[0])\n",
    "        rows[m] = [float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])]\n",
    "\n",
    "if not all(k in rows for k in (1, 2, 3)):\n",
    "    raise RuntimeError(\n",
    "        \"Could not parse the 3 rotation-matrix rows from TM-align -m output.\\n\"\n",
    "        \"First 30 non-empty lines:\\n\" + \"\\n\".join(lines[:30])\n",
    "    )\n",
    "\n",
    "t = [rows[1][0], rows[2][0], rows[3][0]]\n",
    "U = [\n",
    "    [rows[1][1], rows[1][2], rows[1][3]],\n",
    "    [rows[2][1], rows[2][2], rows[2][3]],\n",
    "    [rows[3][1], rows[3][2], rows[3][3]],\n",
    "]\n",
    "\n",
    "# TM-align: X2 = t + U * x1  (Chain_1 -> Chain_2)\n",
    "# We want to move B onto A => apply inverse transform to B:\n",
    "UT = [\n",
    "    [U[0][0], U[1][0], U[2][0]],\n",
    "    [U[0][1], U[1][1], U[2][1]],\n",
    "    [U[0][2], U[1][2], U[2][2]],\n",
    "]\n",
    "t_inv = [\n",
    "    -(UT[0][0]*t[0] + UT[0][1]*t[1] + UT[0][2]*t[2]),\n",
    "    -(UT[1][0]*t[0] + UT[1][1]*t[1] + UT[1][2]*t[2]),\n",
    "    -(UT[2][0]*t[0] + UT[2][1]*t[1] + UT[2][2]*t[2]),\n",
    "]\n",
    "\n",
    "M_inv = [\n",
    "    UT[0][0], UT[0][1], UT[0][2], t_inv[0],\n",
    "    UT[1][0], UT[1][1], UT[1][2], t_inv[1],\n",
    "    UT[2][0], UT[2][1], UT[2][2], t_inv[2],\n",
    "    0,        0,        0,        1\n",
    "]\n",
    "\n",
    "# --- Write B superposed onto A (for NGLView) ---------------------------------\n",
    "B_SUP = WORK_DIR / \"B_superposed.pdb\"\n",
    "\n",
    "def apply_transform_to_structure(in_pdb: Path, out_pdb: Path, M):\n",
    "    st = gemmi.read_structure(str(in_pdb))\n",
    "    r11,r12,r13,t1, r21,r22,r23,t2, r31,r32,r33,t3, _,_,_,_ = M\n",
    "    for model in st:\n",
    "        for chain in model:\n",
    "            for res in chain:\n",
    "                for atom in res:\n",
    "                    x,y,z = atom.pos.x, atom.pos.y, atom.pos.z\n",
    "                    X = t1 + r11*x + r12*y + r13*z\n",
    "                    Y = t2 + r21*x + r22*y + r23*z\n",
    "                    Z = t3 + r31*x + r32*y + r33*z\n",
    "                    atom.pos = gemmi.Position(X, Y, Z)\n",
    "    st.write_pdb(str(out_pdb))\n",
    "\n",
    "apply_transform_to_structure(pdb_b, B_SUP, M_inv)\n",
    "\n",
    "# --- Optional: 3Dmol (only if requested) -------------------------------------\n",
    "if SHOW_3DMOL:\n",
    "    import py3Dmol\n",
    "    v3 = py3Dmol.view(width=900, height=520)\n",
    "    v3.addModel(pdb_a.read_text(), \"pdb\")\n",
    "    v3.addModel(pdb_b.read_text(), \"pdb\")\n",
    "    v3.setTransform({\"model\": 1}, M_inv)\n",
    "    v3.setStyle({\"model\": 0}, {\"cartoon\": {\"color\": \"dodgerblue\"}})\n",
    "    v3.setStyle({\"model\": 1}, {\"cartoon\": {\"color\": \"tomato\", \"opacity\": 0.70}})\n",
    "    v3.zoomTo()\n",
    "    v3  # render as output (no .show())\n",
    "\n",
    "# Superposition viewer (NGLView, force uniform colors; defeat rainbow defaults)\n",
    "\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "!pip -q install nglview gemmi\n",
    "\n",
    "import nglview as nv\n",
    "from pathlib import Path\n",
    "\n",
    "WORK_DIR = Path(\"/content/tmalign_work\").resolve()\n",
    "pdb_a = WORK_DIR / \"A.pdb\"\n",
    "b_sup = WORK_DIR / \"B_superposed.pdb\"\n",
    "\n",
    "if not pdb_a.exists():\n",
    "    raise FileNotFoundError(pdb_a)\n",
    "if not b_sup.exists():\n",
    "    raise FileNotFoundError(b_sup)\n",
    "\n",
    "view = nv.NGLWidget()\n",
    "\n",
    "# Add components\n",
    "compA = view.add_component(str(pdb_a))\n",
    "compB = view.add_component(str(b_sup))\n",
    "\n",
    "compA.clear_representations()\n",
    "compB.clear_representations()\n",
    "\n",
    "\n",
    "compA.add_cartoon(colorScheme=\"uniform\", color=\"red\")\n",
    "compB.add_cartoon(colorScheme=\"uniform\", color=\"blue\", opacity=0.7)\n",
    "\n",
    "view.center()\n",
    "view"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
